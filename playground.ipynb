{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teacher_query_tools import ANLITeacherQuerier, CQATeacherQuerier, ESNLITeacherQuerier, SVAMPTeacherQuerier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Querier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anliTQ = ANLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yml = anliTQ.read_yaml_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'anli1',\n",
       " 'templates': {1: {'id': 1,\n",
       "   'system_message': '',\n",
       "   'user_message': '{premise} Based on that information, is the claim: {hypothesis} true, false, or inconclusive? Answer with a one sentence explanation.',\n",
       "   'label_parse': \"r'(True|False|Inconclusive)'\",\n",
       "   'explanation': True},\n",
       "  2: {'id': 2,\n",
       "   'system_message': 'You are given a context in the form of a short premise and a hypothesis about the premise. Your task is to label if the hypothesis is a \"contradiction\" (if the hypothesis contradicts the premise), an \"entailment\" (if the hypothesis entails the premise), or \"neutral\" (if the hypothesis does not contradict or entail the premise) to the premise. Also, explain very briefly (one sentence, maximum twenty words) why it is that label, but do not repeat the whole hypothesis in your explanation.',\n",
       "   'user_message': 'premise: {premise}\\\\nhypothesis: {hypothesis}',\n",
       "   'label_parse': \"r'(entailment|contradiction|neutral)'\",\n",
       "   'explanation': True},\n",
       "  3: {'id': 3,\n",
       "   'system_message': '',\n",
       "   'user_message': '{premise} Based on that information, is the claim: {hypothesis} true, false, or inconclusive? Answer without an explanation.',\n",
       "   'label_parse': \"r'(True|False|Inconclusive)'\",\n",
       "   'explanation': False}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alexandra Lendon Bastedo (9 March 1946 â€“ 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. Based on that information, is the claim: Bastedo didn't keep any pets because of her views on animal rights. true, false, or inconclusive? Answer with a one sentence explanation.\n",
      "RESPONSE:\n",
      "Inconclusive, as there is no information provided to confirm or deny whether Bastedo kept any pets despite her views on animal rights.\n"
     ]
    }
   ],
   "source": [
    "anliTQ._query(split=\"train\", idx=2, prompt_template_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 15/15...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 1884\n",
      "Total Completion Tokens: 421\n",
      "Total Costs: $0.0036679999999999994\n"
     ]
    }
   ],
   "source": [
    "anliTQ._batch_query(split=\"train\", n=15, prompt_template_id=1, force_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svampTQ = SVAMPTeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 10/10...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 629\n",
      "Total Completion Tokens: 1235\n",
      "Total Costs: $0.0034135000000000003\n"
     ]
    }
   ],
   "source": [
    "svampTQ._batch_query(split=\"train\", n=10, prompt_template_id=2, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli = ESNLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 10/10...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 560\n",
      "Total Completion Tokens: 28\n",
      "Total Costs: $0.0008960000000000001\n"
     ]
    }
   ],
   "source": [
    "esnli._batch_query(split=\"train\", n=10, prompt_template_id=3, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqaTQ = CQATeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 10/10...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 676\n",
      "Total Completion Tokens: 296\n",
      "Total Costs: $0.0016060000000000002\n"
     ]
    }
   ],
   "source": [
    "cqaTQ._batch_query(split=\"train\", n=10, prompt_template_id=2, force_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teacher_response_parser import SVAMPTeacherResponseParser, ESNLITeacherResponseParser, CQATeacherResponseParser, ANLITeacherResponseParser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ANLITeacherResponseParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neutral', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"(\\d+)\", re.IGNORECASE|re.DOTALL)\n",
    "parser.parse_response(\"In each row, there are 30 crayons, so in 7 rows there are 7 * 30 = <<7*30=210>>210 crayons. Answer: \\\\boxed{210}.\", pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(.+\\\\b)(\\d+)\")\n",
    "match = pattern.search(\"In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{210}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{',\n",
       " '210')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ('false',\n",
       "  'the claim that the trolleybus system has over 2 urban routes is false, as the system presently comprises four urban routes.'),\n",
       " 1: ('false',\n",
       "  'Sharron Macready was a popular character in the 1960s, not the 1980s.'),\n",
       " 2: ('inconclusive',\n",
       "  'The information provided does not specifically mention whether Bastedo kept pets or not.'),\n",
       " 3: ('inconclusive',\n",
       "  'The given information does not provide any details about how Alexandra Bastedo was named, so it cannot be determined whether her mother named her or not.'),\n",
       " 4: ('inconclusive',\n",
       "  'While Bastedo was an animal welfare advocate and vegetarian, it cannot be definitively concluded that she cared for all animals that inhabit the earth.'),\n",
       " 5: ('true',\n",
       "  'as the information provided states that Bastedo was a vegetarian, indicating that she never ate meat in her life.'),\n",
       " 6: ('false',\n",
       "  'Based on the information provided, Jesse James was a guerrilla fighting for the Confederacy, not the Union army.'),\n",
       " 7: ('true',\n",
       "  'Rhodochiton atrosanguineus is indeed a species of the family Plantaginaceae based on the provided information.'),\n",
       " 8: ('true', None),\n",
       " 9: ('false',\n",
       "  \"The claim does not provide any information about Paul Beard's involvement in songwriting or his musical compositions, so it cannot be determined whether he wrote his last song in 1989 based on the given information.\"),\n",
       " 10: ('false',\n",
       "  \"The information provided does not mention anything about Paul Beard writing a song at Sir Adrian Boult's BBC Symphony Orchestra.\"),\n",
       " 11: ('true',\n",
       "  'Rhodochiton atrosanguineus is native to southern Mexico and neighbouring Guatemala, indicating that it is native to more than one country.'),\n",
       " 12: ('inconclusive',\n",
       "  'as the information provided does not state the salary of Paul Beard or compare it to other members of the orchestra.'),\n",
       " 13: ('true',\n",
       "  'The purple bell vine, Rhodochiton atrosanguineus, is native to southern Mexico and neighbouring Guatemala, indicating that it can be found in more than one country.'),\n",
       " 14: ('true',\n",
       "  'Paul Beard was a teacher, holding posts at the Royal Academy of Music and the Guildhall School of Music, which suggests that he taught for a significant portion of his life.')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_response_batch(split=\"train\", prompt_template_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "s.format(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})\n",
    "#(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
