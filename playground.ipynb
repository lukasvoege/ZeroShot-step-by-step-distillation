{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\MasterThesis\\ZeroShot-step-by-step-distillation\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.teacher_query_tools import ANLITeacherQuerier, CQATeacherQuerier, ESNLITeacherQuerier, SVAMPTeacherQuerier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Querier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anliTQ = ANLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Premise: The Parma trolleybus system (Italian: \"Rete filoviaria di Parma\" ) forms part of the public transport network of the city and \"comune\" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.\\nClaim: The trolleybus system has over 2 urban routes\\n\\nWhy is the hypothesis \"entailment\" towards the premise? Answer in a single sentence starting with: \"The hypothesis is \"entailment\" towards the premise, because...\"\n",
      "RESPONSE:\n",
      "The hypothesis is \"entailment\" towards the premise, because it directly supports and confirms the information provided in the premise that the Parma trolleybus system presently comprises four urban routes.\n"
     ]
    }
   ],
   "source": [
    "anliTQ._query(split=\"train\", idx=0, prompt_template_id=179, dont_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 10/10 (9)...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 1438\n",
      "Total Completion Tokens: 390\n",
      "Total Costs: $0.002937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1438, 390, 0.002937)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anliTQ._batch_query(split=\"train\", idxs=list(range(10)), prompt_template_id=179, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svampTQ = SVAMPTeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Query completed! (Skipped 30 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 566\n",
      "Total Completion Tokens: 635\n",
      "Total Costs: $0.002119\n"
     ]
    }
   ],
   "source": [
    "svampTQ._batch_query(split=\"train\", idxs=list(range(41)), prompt_template_id=1, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli = ESNLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Query completed! (Skipped 2 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 0\n",
      "Total Completion Tokens: 0\n",
      "Total Costs: $0\n"
     ]
    }
   ],
   "source": [
    "esnli._batch_query(split=\"train\", idxs=[0, 34191], prompt_template_id=1, force_query=False, dont_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqaTQ = CQATeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Query completed! (Skipped 3 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 148\n",
      "Total Completion Tokens: 58\n",
      "Total Costs: $0.000338\n"
     ]
    }
   ],
   "source": [
    "cqaTQ._batch_query(split=\"train\", idxs=[0, 1, 2, 100, 200], prompt_template_id=2, force_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.teacher_response_parser import SVAMPTeacherResponseParser, ESNLITeacherResponseParser, CQATeacherResponseParser, ANLITeacherResponseParser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ANLITeacherResponseParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lol',\n",
       " 'fire because coal is commonly used as a fuel source to ignite and sustain fires.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"(balls|a\\)|b\\)|c\\)|d\\)|e\\)|1\\)|2\\)|3\\)|4\\)|5\\)|1\\.|2\\.|3\\.|4\\.|5\\.).?(.*)\", re.IGNORECASE|re.DOTALL)\n",
    "parser.parse_response(\"The correct answer is a) fire because coal is commonly used as a fuel source to ignite and sustain fires.\", pattern=pattern, prompt_values={\"choice_a\": \"lol\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(.+\\\\b)(\\d+)\")\n",
    "match = pattern.search(\"In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{210}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ('entailment',\n",
       "  'it directly supports and confirms the statement that the trolleybus system in Parma presently comprises four urban routes, which is consistent with the claim that it has over 2 urban routes.'),\n",
       " 1: ('neutral',\n",
       "  'it does not make any claims or express any opinions about the popularity of the character Sharron Macready in the 1980s.'),\n",
       " 2: ('neutral',\n",
       "  \"it does not make any assumptions or provide any evidence either supporting or refuting the claim that Bastedo didn't keep any pets due to her views on animal rights.\"),\n",
       " 3: ('neutral',\n",
       "  'it does not make any specific claims about whether Alexandra Bastedo was named by her mother or not.'),\n",
       " 4: ('neutral',\n",
       "  \"it does not provide any evidence or support either confirming or refuting Bastedo's care for all animals.\"),\n",
       " 5: ('neutral',\n",
       "  'it does not make a definitive statement about whether Bastedo never ate meat in her life.'),\n",
       " 6: ('contradiction',\n",
       "  'the premise states that Jesse James was a guerrilla leader for the Confederacy, while the hypothesis claims that he was a guerrilla in the Union army.'),\n",
       " 7: ('entailment',\n",
       "  'it directly supports and aligns with the information provided in the premise, stating that Rhodochiton atrosanguineus is indeed a species within the family Plantaginaceae.'),\n",
       " 8: ('neutral',\n",
       "  'it does not make any judgment or assumption about whether or not Paul Beard started as the leader of the London Philharmonic Orchestra.'),\n",
       " 9: ('neutral',\n",
       "  'it does not make any claims or assertions about whether Paul Beard wrote his last song in 1989 or not.'),\n",
       " 5333: ('neutral',\n",
       "  'it does not make any claims or provide any evidence either supporting or refuting the premise.'),\n",
       " 8443: ('neutral',\n",
       "  'it does not provide any information or stance on whether The Living and the Dead was renewed for 7 seasons or not.'),\n",
       " 16741: ('contradiction',\n",
       "  'film studio 20th Century Fox Film Corporation, while the premise clearly states that the films listed are produced by the U.S. film studio 20th Century Fox Film Corporation.'),\n",
       " 735: ('contradiction',\n",
       "  'it states that the South Kalgoorlie Gold Mine is known as the North Kal Mines, which directly contradicts the information provided in the premise.'),\n",
       " 7559: ('contradiction',\n",
       "  \"it states that James Joyce published The Cats of Copenhagen, whereas the premise states that it was published posthumously after Joyce's work entered the public domain.\"),\n",
       " 16652: ('contradiction',\n",
       "  'it states that Ward retired in his 8th season, while the premise clearly states that he played 9 seasons in Major League Baseball.'),\n",
       " 13064: ('neutral',\n",
       "  'it does not make any claims or assumptions about whether or not Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London.'),\n",
       " 11538: ('entailment', 'Elrod, who was a Marine aviator.'),\n",
       " 13776: ('neutral',\n",
       "  \"it does not have a preconceived opinion or bias about whether or not World Premiere was Ted Tetzlaff's directorial debut.\"),\n",
       " 2325: ('neutral',\n",
       "  'it does not make a judgment or take a stance on whether or not The Savage Detectives is told from the viewpoint of two poets.'),\n",
       " 8253: ('neutral',\n",
       "  'it does not make any explicit judgment or stance on whether Incline Village was designed as a community for Native Americans.'),\n",
       " 10684: ('neutral',\n",
       "  'it simply restates the main plot of the film without expressing any opinion or bias.'),\n",
       " 16845: ('contradiction',\n",
       "  'Theodora Kracaw Kroeber Quinn died at the age of 82, not over a hundred years old.'),\n",
       " 9044: ('neutral',\n",
       "  \"it does not make any claims or assumptions about the consumption of Killian's red beer by Irish people based on the information provided.\"),\n",
       " 8094: ('entailment',\n",
       "  'it directly supports and confirms the claim that Logo TV shifted its focus towards general cultural and lifestyle programming in 2012.'),\n",
       " 6442: ('neutral',\n",
       "  'it does not provide any evidence or indication either supporting or contradicting the claim that Hans im Glück published five board games in 1997.'),\n",
       " 9004: ('neutral',\n",
       "  'it does not provide any information or opinion on whether the Transporter television series was indeed filmed in Vancouver.'),\n",
       " 3429: ('neutral',\n",
       "  'it does not make a statement about whether Alexander enjoys being a politician or not.'),\n",
       " 6333: ('contradiction',\n",
       "  'it directly contradicts the statement that Mark Rylance appeared in the movie Institute Benjamenta.'),\n",
       " 12998: ('contradiction',\n",
       "  'the Peace Palace is designed to house educational and meditational activities, not specifically for worship.'),\n",
       " 3589: ('neutral',\n",
       "  'it does not make a specific claim about the language of the dialogue in Happy Go Lovely.'),\n",
       " 1210: ('contradiction',\n",
       "  'Tito Puente, the mambo musician and percussionist after whom the Tito Puente Amphitheatre is named, is deceased and therefore cannot be alive today to perform concerts at the venue.'),\n",
       " 16922: ('contradiction',\n",
       "  'Kyle Schickner, the founder of FenceSitter Films, is a bisexual civil rights activist, indicating support for the rights of sexual minorities, including same-sex marriage.'),\n",
       " 6422: ('neutral',\n",
       "  'it does not express agreement or disagreement with the claim that Edward George \"Ed\" Skrein was born in London, England.'),\n",
       " 2021: ('neutral',\n",
       "  'it does not make any claims or assumptions about the income of Leander Paes and Nenad Zimonjić.'),\n",
       " 14365: ('neutral',\n",
       "  \"it neither supports nor contradicts the claim that Rabbi Levi Yitzchok of Berditchev was Rabbi Dovid Schlomo Novoseller's grandfather.\"),\n",
       " 7695: ('entailment',\n",
       "  'it directly supports the statement that \"Movin\\' On Up\" is the first track off of Primal Scream\\'s album \"Screamadelica\".'),\n",
       " 3703: ('neutral',\n",
       "  'it neither supports nor refutes the claim that Joy Womack speaks Russian, as it does not provide any information or stance on her language abilities.'),\n",
       " 14783: ('entailment',\n",
       "  \"it directly supports and follows from the information provided about George Washington Baines, Sr.'s roles as a natural science professor and President of Baylor University at its first location in Independence.\"),\n",
       " 10591: ('entailment',\n",
       "  'it contradicts the premise by stating that Miss Peru 2008 was held in a different year than stated.'),\n",
       " 16430: ('contradiction',\n",
       "  'the Parma trolleybus system was not in operation until 1953, which is after the 1940s.'),\n",
       " 11505: ('neutral',\n",
       "  'it does not affirm or deny the claim that William Gurstelle is known for his contributions at \"Popular Science\" magazine by all scientists.'),\n",
       " 10362: ('entailment',\n",
       "  'the premise explicitly states that Arthur \"Skip\" Roderick played professionally in the Major Indoor Soccer League, which indicates that he played soccer indoors.'),\n",
       " 6844: ('contradiction',\n",
       "  \"it states that Dyllan McGee's shows are not on TV, whereas the premise clearly states that she has produced shows for HBO, PBS, and AOL.\"),\n",
       " 16644: ('contradiction', 'JAM8 is the composer of the song, not the singer.'),\n",
       " 1493: ('contradiction',\n",
       "  'it states that the current Isanti County Courthouse is located at the same address as the former courthouse, when in fact the former courthouse has been remodeled into a private office building.'),\n",
       " 123: ('entailment',\n",
       "  'it directly supports the statement that Miroslav Ondříček was born in Prague, as mentioned in the premise.'),\n",
       " 11055: ('neutral',\n",
       "  'it does not make a judgment about whether Charlotte Anley was actually against the treatment of women in prisons.'),\n",
       " 5834: ('neutral',\n",
       "  'it does not make any specific claims or statements about the premise being true or false.'),\n",
       " 15287: ('entailment',\n",
       "  'it directly supports and confirms the information provided in the premise that Abesim is indeed located in the Brong-Ahafo Region of Ghana.'),\n",
       " 12745: ('neutral',\n",
       "  'it does not make a definitive statement about the popularity of the Aurora Sub, leaving room for further investigation and evidence to support or refute its popularity.'),\n",
       " 4074: ('entailment',\n",
       "  'it directly supports and aligns with the information provided in the premise that Fußball Club Südburgenland has indeed played in the ÖFB-Frauenliga since the 2003-04 season.'),\n",
       " 3599: ('neutral',\n",
       "  \"it does not make any judgment or claim about the fame or popularity of Bailey's compared to other Irish cream brands.\"),\n",
       " 2099: ('contradiction',\n",
       "  'it states that The Shins are an indie rock band from Portland, Oregon, which contradicts the premise that they are originally from Albuquerque, New Mexico and only based in Portland.'),\n",
       " 7026: ('entailment',\n",
       "  \"it accurately reflects the chronological sequence of events and the timeline of Tear Out The Heart's album releases.\"),\n",
       " 3617: ('entailment',\n",
       "  'it directly restates the information provided in the premise, confirming that \"Can\\'t Leave \\'em Alone\" is indeed an R&B song written by Ciara, LaShawn Daniels, Rodney Jerkins, and 50 Cent.'),\n",
       " 322: ('entailment',\n",
       "  'the claim directly contradicts the information provided in the premise about Fred Berger\\'s achievements and awards for \"La La Land\".'),\n",
       " 12654: ('entailment',\n",
       "  \"it directly supports and confirms the statement that Editors currently has five band members by providing specific information about each member's role in the band.\"),\n",
       " 4417: ('contradiction',\n",
       "  'it claims that Joseph Groome Tower is one thirteen-story building, while the premise states that there are actually three thirteen-storey tower blocks.'),\n",
       " 7048: ('entailment',\n",
       "  'the premise explicitly states that Royal Unibrew owns breweries in Lithuania, which confirms that Lithuania is indeed in the Baltic region.'),\n",
       " 4755: ('neutral',\n",
       "  \"it does not have any relevant information or opinion regarding Julia Stiles' age when she starred in The Prince and Me in 2004.\"),\n",
       " 10815: ('neutral',\n",
       "  'it does not make any claims or assumptions about the size of Fulletby in comparison to other villages in the East Lindsey district.'),\n",
       " 2420: ('entailment',\n",
       "  \"it directly supports the claim that South Sea Rose was made before 1930 by providing specific details about the film's distribution, production, and collaboration between the director and star.\"),\n",
       " 11255: ('entailment',\n",
       "  'it directly supports and aligns with the claim that Paul Gustavson was the creator of the Angel character in Marvel Comics.'),\n",
       " 2266: ('entailment',\n",
       "  'it directly corresponds to and supports the information given in the premise, stating that Sha Na Na began in 1969.'),\n",
       " 3434: ('neutral',\n",
       "  'it does not make any judgment or take a stance on whether the second NASCAR Sprint Cup Series was held in May or not.'),\n",
       " 608: ('contradiction',\n",
       "  'it states that \"Howard Sings Ashman\" is a three-disc album, while the premise clearly states that it is a two-disc album.'),\n",
       " 11643: ('contradiction',\n",
       "  'it states that Say Anything was released over 50 years ago, while the premise clearly states that it was released in 2008.'),\n",
       " 3035: ('entailment',\n",
       "  'it directly supports and aligns with the claim that Cooking Light, a magazine founded in 1987, includes approximately 100 original recipes each month.'),\n",
       " 5908: ('entailment',\n",
       "  'it directly supports and aligns with the statement that Little Jost Van Dyke is one of the British Virgin Islands and provides additional information about its location and the Diamond Cay National Park.'),\n",
       " 59: ('neutral',\n",
       "  'it does not make a subjective judgment or provide any evidence to support the claim that Dorobantu is the best commune.'),\n",
       " 4489: ('contradiction',\n",
       "  'the premise clearly states that the BSF program aims to help couples fulfill their aspirations for a healthy relationship and marriage, while the hypothesis suggests that the program is intended to expedite the divorce process.'),\n",
       " 16436: ('contradiction',\n",
       "  'Janez Pišek passed away and is unable to present the Blodek plaque in 2020.'),\n",
       " 9750: ('neutral',\n",
       "  'it does not make any assumptions or judgments about where Brett McLaughlin was born.'),\n",
       " 15298: ('entailment',\n",
       "  'it directly supports and aligns with the claim that the Whitewater scandal involved Presidents of the US, specifically Bill and Hillary Clinton.'),\n",
       " 11919: ('neutral',\n",
       "  \"it does not make any judgments or assumptions about the accuracy or validity of Caroline Milmoe's birth date or the weather conditions on that day.\"),\n",
       " 7171: ('entailment',\n",
       "  'it directly supports the statement that Sara Zarr is an American writer.'),\n",
       " 15687: ('entailment',\n",
       "  \"it directly supports and aligns with the information provided in the premise about the timing and nature of the Deuxiéme Meeting International d'Éducation Physique Féminine.\"),\n",
       " 12097: ('contradiction',\n",
       "  'it falsely claims that the United States Government Manual is now known as the United States Government Organization Manual, when in fact, it was known by that name prior to the 1973/74 edition.'),\n",
       " 2861: ('neutral',\n",
       "  'it neither supports nor opposes the claim that Evans Tuning, LLC is located in the United States.'),\n",
       " 16245: ('neutral',\n",
       "  'it does not make any judgment or express a bias towards the length of the second season of South Park.'),\n",
       " 3090: ('neutral',\n",
       "  'it does not make any specific claims or judgments about whether the director of the original movie was involved in directing the television show.'),\n",
       " 16076: ('neutral',\n",
       "  'it does not make any judgment or provide any opinion about whether Patrick Brontë was an Irish priest during 1861.'),\n",
       " 450: ('entailment',\n",
       "  'it directly supports and aligns with the statement that John Forsythe was indeed an American stage, film/television actor.'),\n",
       " 6334: ('neutral',\n",
       "  'it does not take a stance on whether Yakshagana is a difficult theatrical form to master or not.'),\n",
       " 3421: ('neutral',\n",
       "  'it does not make any assumptions or judgments about whether or not Kiecal had boxing matches as an amateur.'),\n",
       " 7305: ('contradiction',\n",
       "  'it states that Old Trafford opened in 1847 as the home of Manchester Cricket Club, while the premise states that it opened in 1857.'),\n",
       " 15648: ('entailment',\n",
       "  'the premise states that Paul Williams and Pat McCormick were reunited after their pairing in the \"Smokey and the Bandit\" movies, implying that they had worked together on multiple movie projects in their respective careers.'),\n",
       " 15798: ('entailment',\n",
       "  'it directly relates to the claim made in the premise that \"Parallel Worlds\" is the debut single from Elliot Minor\\'s self-titled album, implying that there is only one track on the single.'),\n",
       " 14569: ('contradiction',\n",
       "  \"it directly states that Tartuffe is French for 'the hypocrite,' while the premise states that Tartuffe is the name of a play and one of the characters in the play.\"),\n",
       " 13426: ('neutral',\n",
       "  'it does not make a judgment on whether Taylor Swift is the first artist to write a song about the joys of being 22 years old.'),\n",
       " 3704: ('contradiction',\n",
       "  'it states that the UK release came after the US release, while the premise suggests that \"Touch\" is essentially a clone of the North American release \"Audio Lessonover?\".'),\n",
       " 4058: ('neutral',\n",
       "  'it does not make any assumptions or assertions about whether Robert Jack Duarte Wallace has a brother or not.'),\n",
       " 11610: ('neutral',\n",
       "  'it does not take a stance on whether Jamiel Shaw II was a High School football quarterback or not.'),\n",
       " 12930: ('neutral',\n",
       "  'it does not take a position on whether or not Alvarez won the titles from the fight.'),\n",
       " 7438: ('neutral',\n",
       "  'it does not make a definitive statement about whether most Corydalis species are perennials or not.'),\n",
       " 8009: ('entailment',\n",
       "  'it directly supports and aligns with the statement that Jia Zhangke is a part of the Sixth Generation movement of Chinese cinema.'),\n",
       " 16854: ('contradiction',\n",
       "  'Sean Hannity was born in 1961 and could not have been a talk show host since his birth.'),\n",
       " 10370: ('neutral',\n",
       "  'it does not make any specific claims or assumptions about whether Bill Lowrey has played the four-string banjo since he was thirteen.'),\n",
       " 15751: ('entailment',\n",
       "  'it directly supports and corroborates the claim that the Hill 50 Gold Mine was indeed bought in 2010.'),\n",
       " 6561: ('contradiction',\n",
       "  'the premise clearly states that the game has no relation to the 1969 film, while the hypothesis suggests that the game is based on the movie.'),\n",
       " 11652: ('neutral',\n",
       "  'it does not make a specific claim about the financial success of The Jungle Book film.'),\n",
       " 1090: ('neutral',\n",
       "  'it does not make any specific claims or predictions about the growth of Achillea ageratum in France.'),\n",
       " 7809: ('neutral',\n",
       "  \"it does not express a judgment or opinion on whether James Joyce's Women was considered a magnum opus or not.\"),\n",
       " 6758: ('neutral',\n",
       "  \"it does not make any specific claims or judgments about Daniel Kaufmann's educational background or the prestige of the university he attended.\"),\n",
       " 11395: ('entailment',\n",
       "  'it states that Emma Rigby is mostly known for being on Hollyoaks, which is supported by the premise stating that she is best known for playing the role of Hannah Ashworth in the long-running soap opera \"Hollyoaks.\"'),\n",
       " 6156: ('neutral',\n",
       "  \"it does not make any specific claims or assumptions about James Forbes Chapin's musical background or upbringing.\"),\n",
       " 10172: ('neutral',\n",
       "  'it does not make any assumptions or judgments about whether Mushfiqur Rahman Gulzar has directed a film based on the given information.'),\n",
       " 9187: ('entailment',\n",
       "  'it directly supports and confirms the statement that Peter Chelsom was born on April 20th, 1956, which is stated in the premise.'),\n",
       " 2467: ('contradiction',\n",
       "  'Open in 1973, but it does not mention her winning the tournament.')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_response_batch(split=\"train\", prompt_template_id=179, verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{',\n",
       " '210')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ('42', None),\n",
       " 1: ('210', None),\n",
       " 2: ('34', 'Julia played with a total of'),\n",
       " 3: ('6', None),\n",
       " 4: ('7', None),\n",
       " 5: ('1088', 'The mailman should give a total of'),\n",
       " 6: ('0.5', 'Each person ate 0.5 crackers and'),\n",
       " 7: ('280', None),\n",
       " 8: ('5', None),\n",
       " 9: ('13', None),\n",
       " 10: ('12', None),\n",
       " 11: ('150', None),\n",
       " 12: ('60', None),\n",
       " 13: ('69', None),\n",
       " 14: ('9', None),\n",
       " 15: ('204', 'The farmer picked a total of'),\n",
       " 16: ('180', 'The machine made a total of'),\n",
       " 17: ('18', None),\n",
       " 18: ('18', 'The number of birds sitting on the fence is'),\n",
       " 19: ('8', None),\n",
       " 20: ('2', None),\n",
       " 21: ('60', 'The depth of the water was'),\n",
       " 22: ('66', 'The grasshopper and the frog jumped a total of'),\n",
       " 23: ('89', None),\n",
       " 24: ('130', None),\n",
       " 25: ('2', None),\n",
       " 26: ('32', None),\n",
       " 27: ('154', None),\n",
       " 28: ('345', None),\n",
       " 29: ('1', None),\n",
       " 696: ('2', None),\n",
       " 667: ('4', None),\n",
       " 63: ('12', None),\n",
       " 533: ('26', None),\n",
       " 66: ('3', None),\n",
       " 621: ('51', None),\n",
       " 346: ('7', None),\n",
       " 490: ('238', None),\n",
       " 760: ('4', None),\n",
       " 456: ('1', None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_response_batch(split=\"train\", prompt_template_id=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "s.format(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})\n",
    "#(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = utils.Metadata()\n",
    "c2 = utils.OtherMeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another static'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.static_thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.teacher_response_evaluator import TeacherResponseEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = TeacherResponseEvaluator(\"esnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.76,\n",
       " 'n_correct': 38,\n",
       " 'n_wrong': 12,\n",
       " 'n_parse_errors': 0,\n",
       " 'n_none_responses': 0,\n",
       " 'total_responses': 50,\n",
       " 'total_length_of_explanations': 7145}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_responses_split(split = \"train\", prompt_template_id = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse '8 customers left than those that stayed behind.' with pattern 're.compile('(.+[\\\\s{\\\\$â‚¬])(\\\\d*\\\\.?\\\\d+)', re.IGNORECASE|re.DOTALL)'\n",
      "Could not parse '137 more students suggested mashed potatoes than those that suggested bacon.' with pattern 're.compile('(.+[\\\\s\\\\{\\\\$â‚¬])(\\\\d*\\\\.?\\\\d+)', re.IGNORECASE|re.DOTALL)'\n",
      "Could not parse '34 bird families were living near the mountain at the start.' with pattern 're.compile('(.+[\\\\s\\\\{\\\\$â‚¬])(\\\\d*\\\\.?\\\\d+)', re.IGNORECASE|re.DOTALL)'\n",
      "Could not parse '65 more students suggested mashed potatoes than those that suggested tomatoes.' with pattern 're.compile('(.+[\\\\s\\\\{\\\\$â‚¬])(\\\\d*\\\\.?\\\\d+)', re.IGNORECASE|re.DOTALL)'\n",
      "Could not parse '30 campers went rowing and hiking in all.' with pattern 're.compile('(.+[\\\\s\\\\{\\\\$â‚¬])(\\\\d*\\\\.?\\\\d+)', re.IGNORECASE|re.DOTALL)'\n",
      "Best prompt template for svamp is 2, with accuracy: 0.7936507936507936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teacher_writer import TeacherWriter\n",
    "tw = TeacherWriter(\"svamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = {\"label\": {1: range(10)}, \"explanation\": {2: range(10)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw.write_teacher_responses(split=\"train\", prompt_template_id_mix=mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! As an AI, I don't have emotions, but I'm here to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOpenAI(model = \"gpt-3.5-turbo\", temperature=1.1, max_tokens=200, request_timeout=10)\n",
    "\n",
    "chat_model.predict(\"Hi, how are you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample ESNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\master-thesis\\ZeroShot-step-by-step-distillation\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "dsbs_du = importlib.import_module(\"distilling-step-by-step.data_utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dsbs_du.ESNLIDatasetLoader(subset=\"small\")\n",
    "datasets = dataloader.load_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"] = datasets[\"train\"].train_test_split(test_size=0.6909)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"test\"] = datasets[\"test\"].train_test_split(test_size=0.8982)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"valid\"] = datasets[\"valid\"].train_test_split(test_size=0.89839)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "to_write = \"\"\n",
    "with open(f\"esnli_valid.json\", \"w\") as f:\n",
    "    for line in datasets[\"valid\"]:\n",
    "        json.dump(line, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{write_location}/{split}_CoT.json\", \"w\") as f:\n",
    "    for line in to_write:\n",
    "        json.dump(line, f)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
