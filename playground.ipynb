{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\MasterThesis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from teacher_query_tools import ANLITeacherQuerier, CQATeacherQuerier, ESNLITeacherQuerier, SVAMPTeacherQuerier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Querier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anliTQ = ANLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yml = anliTQ.read_yaml_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'anli1',\n",
       " 'templates': {1: {'id': 1,\n",
       "   'system_message': '',\n",
       "   'user_message': '{premise} Based on that information, is the claim: {hypothesis} true, false, or inconclusive? Answer with a one sentence explanation.',\n",
       "   'label_parse': \"r'(True|False|Inconclusive)'\",\n",
       "   'explanation': True},\n",
       "  2: {'id': 2,\n",
       "   'system_message': 'You are given a context in the form of a short premise and a hypothesis about the premise. Your task is to label if the hypothesis is a \"contradiction\" (if the hypothesis contradicts the premise), an \"entailment\" (if the hypothesis entails the premise), or \"neutral\" (if the hypothesis does not contradict or entail the premise) to the premise. Also, explain very briefly (one sentence, maximum twenty words) why it is that label, but do not repeat the whole hypothesis in your explanation.',\n",
       "   'user_message': 'premise: {premise}\\\\nhypothesis: {hypothesis}',\n",
       "   'label_parse': \"r'(entailment|contradiction|neutral)'\",\n",
       "   'explanation': True},\n",
       "  3: {'id': 3,\n",
       "   'system_message': '',\n",
       "   'user_message': '{premise} Based on that information, is the claim: {hypothesis} true, false, or inconclusive? Answer without an explanation.',\n",
       "   'label_parse': \"r'(True|False|Inconclusive)'\",\n",
       "   'explanation': False}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alexandra Lendon Bastedo (9 March 1946 â€“ 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series \"The Champions\". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. Based on that information, is the claim: Bastedo didn't keep any pets because of her views on animal rights. true, false, or inconclusive? Answer with a one sentence explanation.\n",
      "RESPONSE:\n",
      "Inconclusive, as there is no information provided to confirm or deny whether Bastedo kept any pets despite her views on animal rights.\n"
     ]
    }
   ],
   "source": [
    "anliTQ._query(split=\"train\", idx=2, prompt_template_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 25/25...\n",
      "Batch Query completed! (Skipped 24 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 143\n",
      "Total Completion Tokens: 16\n",
      "Total Costs: $0.0002465\n"
     ]
    }
   ],
   "source": [
    "anliTQ._batch_query(split=\"train\", n=25, prompt_template_id=1, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "svampTQ = SVAMPTeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 30/30...\n",
      "Batch Query completed! (Skipped 10 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 973\n",
      "Total Completion Tokens: 1313\n",
      "Total Costs: $0.0040855\n"
     ]
    }
   ],
   "source": [
    "svampTQ._batch_query(split=\"train\", n=30, prompt_template_id=1, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli = ESNLITeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 10/10...\n",
      "Batch Query completed! (Skipped 0 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 560\n",
      "Total Completion Tokens: 28\n",
      "Total Costs: $0.0008960000000000001\n"
     ]
    }
   ],
   "source": [
    "esnli._batch_query(split=\"train\", n=10, prompt_template_id=3, force_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqaTQ = CQATeacherQuerier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERYING EXAMPLE 25/25...\n",
      "Batch Query completed! (Skipped 20 queries as they were already queried and stored.)\n",
      "Total Prompt Tokens: 326\n",
      "Total Completion Tokens: 144\n",
      "Total Costs: $0.000777\n"
     ]
    }
   ],
   "source": [
    "cqaTQ._batch_query(split=\"train\", n=25, prompt_template_id=2, force_query=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teacher_response_parser import SVAMPTeacherResponseParser, ESNLITeacherResponseParser, CQATeacherResponseParser, ANLITeacherResponseParser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CQATeacherResponseParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neutral', None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"(\\d+)\", re.IGNORECASE|re.DOTALL)\n",
    "parser.parse_response(\"In each row, there are 30 crayons, so in 7 rows there are 7 * 30 = <<7*30=210>>210 crayons. Answer: \\\\boxed{210}.\", pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(.+\\\\b)(\\d+)\")\n",
    "match = pattern.search(\"In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{210}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In each row, there are 30 crayons, so in 7 rows there are 7 * 30 <<7*30=210>>210 crayons. Answer: \\\\boxed{',\n",
       " '210')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ('math problem',\n",
       "  'This is an example of a math problem because it involves counting and subtraction.'),\n",
       " 1: ('train station',\n",
       "  'as it is a common location for homeless individuals to seek shelter and it is mentioned that he lives near transportation infrastructure, suggesting a more permanent location than a bus depot or beach.'),\n",
       " 2: ('upright',\n",
       "  'A bad person does not prioritize being honest, acting without pretense, or being morally upright in their behavior.'),\n",
       " 3: ('minnesota', 'because St. Paul is the capital city of Minnesota.'),\n",
       " 4: ('corvette',\n",
       "  'A corvette is a smaller, faster, and more maneuverable naval vessel that can still pack a punch with its armament, making it a suitable alternative to a battleship when speed is prioritized.'),\n",
       " 5: ('canada',\n",
       "  'because beavers are native to Canada and are known for building dams and taking logs in their habitats.'),\n",
       " 6: ('city',\n",
       "  'The grant was given to the city for the renovation, indicating that the fountain was funded by the city government.'),\n",
       " 7: ('in charge of project',\n",
       "  \"The boss may see the employee's ambition as a sign of leadership potential and entrust them with overseeing a project.\"),\n",
       " 8: ('punishment',\n",
       "  'The teacher might give punishment to the boy as a consequence for leaving the line without permission.'),\n",
       " 9: ('marriage',\n",
       "  'The likely ceremony for a bride and groom who are taking care of proposals would be the actual marriage ceremony, as proposals typically lead to the formalization of their union through marriage.'),\n",
       " 10: ('meadow',\n",
       "  'because it is a natural habitat for wildflowers and less likely to have human interruption compared to a garden or bug zapper.'),\n",
       " 11: ('heading north', None),\n",
       " 12: ('meow',\n",
       "  'as it is a form of communication used by cats to express their needs and emotions.'),\n",
       " 13: ('residential area',\n",
       "  'Cats are more likely to be found in residential areas where houses and buildings provide shelter, food sources, and potential hiding spots.'),\n",
       " 14: ('comfortable position',\n",
       "  'because it wants to find a cozy and relaxing spot to sleep.'),\n",
       " 15: ('gather flowers',\n",
       "  'because it is spring and meadows are often filled with colorful and blooming flowers.'),\n",
       " 16: ('aquarium', 'as that is the most common location for a goldfish.'),\n",
       " 17: ('backwards',\n",
       "  'This option best fits the context as it implies the opposite direction, which is the opposite of going forth.'),\n",
       " 18: ('believe in god',\n",
       "  'A computer lacks the ability to possess beliefs or engage in religious or spiritual practices as it is a machine designed for processing and executing tasks based on programmed instructions.'),\n",
       " 19: ('vagina',\n",
       "  'The diaphragm is a form of contraceptive designed to cover and block the cervix in the vagina, preventing sperm from entering the uterus.'),\n",
       " 20: ('ocean',\n",
       "  \"The Earth's surface is mostly covered by the ocean, which accounts for approximately 71% of the globe.\"),\n",
       " 21: ('rural area',\n",
       "  'Cow farms tend to be located in rural areas to minimize the impact of the smell on nearby residential and urban areas.'),\n",
       " 22: ('nursery rhyme',\n",
       "  'This answer refers to the popular nursery rhyme \"Hey Diddle Diddle,\" where a cow jumps over the moon, adding an element of excitement and imagination to an otherwise boring cow.'),\n",
       " 23: ('corner',\n",
       "  'A cube has eight corners, one on each vertex where three edges meet.'),\n",
       " 24: ('toolbox',\n",
       "  'This is the most likely answer as a toolbox is a common place for people to keep their own personal tools, including a saw.')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse_response_batch(split=\"train\", prompt_template_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"Tell me a {adjective} joke about {content}.\"\n",
    "\n",
    "s.format(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})\n",
    "#(**{\"adjective\": \"funny\", \"content\": \"chickens\", \"mashall\": \"mashall\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = utils.Metadata()\n",
    "c2 = utils.OtherMeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'another static'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.static_thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\master-thesis\\ZeroShot-step-by-step-distillation\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from teacher_response_evaluator import TeacherResponseEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = TeacherResponseEvaluator(\"svamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'n_correct': 20,\n",
       " 'n_wrong': 10,\n",
       " 'n_none_responses': 2,\n",
       " 'total_reponses': 28,\n",
       " 'total_length_of_explanations': 5399}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_responses_split(split = \"train\", prompt_template_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best prompt template for svamp is 2, with accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
